{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load for Corteva Code Challenge\n",
    "## This code section is in response to problem #2 - Ingestion\n",
    "The ingestion of the data is being completed using Python 3.9 from a Jupyter Notebook.  This solution was chosen because it allows for documentation along side the code.  The code was created in the notebook using Microsoft Visual Studio Code.  This provided a linter and code formatting utilizing the Jupyter extension.\n",
    "\n",
    "Data is loaded from the flat files to the stage tables of the database.  Once data is processed in the final tables the stage data is deleted.  For a more robust solution an arvhive set of tables might might since.\n",
    "\n",
    "Duplicates are handled by leveraging MySQL unique key constraints and the ON DUPLICATE KEY clause that allows for a similar result to the MERGE used by other RDBMS frameworks.  \n",
    "\n",
    "Logging has been added using the Phython logging module and places the results into a log directory. See set requirements for details on setting up the directories to support this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Section\n",
    "This section contains the imports used for the rest of the code base.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import configparser\n",
    "import logging\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Section\n",
    "This secion setups the configuration for the database, logging, and configuration file.\n",
    "\n",
    "The username and password is retreive from the machines environment variables.  This is just one way to secure the private details.\n",
    "\n",
    "Next the configuration file is defined and the log directory read in.\n",
    "\n",
    "The database connection is defined using a combination of environment and configuration settings.\n",
    "\n",
    "Finally all the SQL statements are included in this section.  These are placed here so they are easily available for review, additions, or modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Database credentials from local environment variables\n",
    "db_user = os.getenv('DBUSER')\n",
    "db_pass = os.getenv('DBPASS')\n",
    "\n",
    "# Load configuration file for lookup of values that can be changed easily\n",
    "config = configparser.ConfigParser()\n",
    "config.read('corteva.ini')\n",
    "\n",
    "# Setup logging \n",
    "log_directory = config['DEFAULT']['log_directory'].strip(\"'\")\n",
    "logging.basicConfig(level=logging.DEBUG, filename=log_directory+'corteva.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')\n",
    "\n",
    " \n",
    "#Create a database connection\n",
    "mydb = mysql.connector.connect(\n",
    " host=config['DATABASE']['host'],\n",
    " user=db_user,\n",
    " password=db_pass,\n",
    " database=config['DATABASE']['database'])\n",
    "\n",
    "# Open a cursor.  \n",
    "# Normally cursor would be opened and closed for each call and placed inside a try\n",
    "# since this is running inside a notebook the cursor will be opened and then closed\n",
    "# in the cleanup section at the end\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "# This section sets up the SQL statements for easy reference\n",
    "sql1 = (\"INSERT INTO CCC_STG_YIELD_DATA (YIELD_FILE_NAME, YEAR_OF_YIELD, YIELD_AMOUNT )\"\n",
    "        \"VALUES (%s, %s, %s)\")\n",
    "colName1 = ['year', 'amount']\n",
    "dataType1 ={'year': str,'amount': int}\n",
    "\n",
    "sql2 = (\"INSERT INTO CCC_STG_WEATHER_STATION_DATA (FILE_NAME, DATE_OF_WEATHER, MAX_DAILY_TEMP,\"\n",
    "        \" MIN_DAILY_TEMP, DAILY_PRECIPITATION) \"\n",
    "        \"VALUES (%s, %s, %s, %s, %s)\")\n",
    "colName2 = ['date', 'max_temp', 'min_temp','precip']\n",
    "dataType2 ={'date': str,'max_temp': int, 'min_temp': int, 'precip': int}\n",
    "\n",
    "sqlDimYear = (\"INSERT INTO CCC_DIM_YEAR(YEAR_KEY, THE_YEAR, YYYY) \"\n",
    "              \"(SELECT DISTINCT NULL, YEAR_OF_YIELD, YEAR_OF_YIELD FROM CCC_STG_YIELD_DATA yd) \"\n",
    "              \"ON DUPLICATE KEY UPDATE YEAR_KEY=YEAR_KEY\")\n",
    "\n",
    "sqlFactYld = (\"INSERT INTO CCC_FACT_CROP_YIELD(YEAR_KEY, CROP_YIELD_AMOUNT) \"\n",
    "              \"(SELECT yd.YEAR_KEY, sy.YIELD_AMOUNT \"\n",
    "              \"FROM CCC_DIM_YEAR yd \"\n",
    "              \"JOIN CCC_STG_YIELD_DATA sy ON sy.YEAR_OF_YIELD = yd.THE_YEAR) \"\n",
    "              \"ON DUPLICATE KEY UPDATE CROP_YIELD_AMOUNT = sy.YIELD_AMOUNT\")\n",
    "\n",
    "sqlStgYldDel = (\"DELETE FROM CCC_STG_YIELD_DATA\")\n",
    "\n",
    "sqlDimDate = (\"INSERT INTO CCC_DIM_DATE(DATE_KEY, THE_DATE, DAY_OF_YEAR, WEEK_OF_YEAR, THE_MONTH, MONTH_NAME, \"\n",
    "              \"THE_QUARTER, QUARTER_NAME, THE_YEAR, DATE_SORT) \"\n",
    "              \"(SELECT DISTINCT NULL, STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d'), DAYOFYEAR(STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d')),\"\n",
    "              \"WEEKOFYEAR(STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d')), MONTH(STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d')), \"\n",
    "              \"MONTHNAME(STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d')), QUARTER(STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d')), \"\n",
    "              \"CONCAT('Q', QUARTER(STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d'))), YEAR(STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d')), \"\n",
    "              \"DATE_OF_WEATHER FROM CCC_STG_WEATHER_STATION_DATA) ON DUPLICATE KEY UPDATE DATE_KEY=DATE_KEY\")\n",
    "\n",
    "sqlDimWeatherStation = (\"INSERT INTO CCC_DIM_WEATHER_STATION(WEATHER_STATION_KEY, WEATHER_STATION_CODE, WEATHER_STATION_NAME) \"\n",
    "                        \"(SELECT DISTINCT NULL, SUBSTR(FILE_NAME, 1,  LOCATE('.txt', FILE_NAME,1)-1),SUBSTR(FILE_NAME, 1,  LOCATE('.txt', FILE_NAME,1)-1)\"\n",
    "                        \"FROM CCC_STG_WEATHER_STATION_DATA) ON DUPLICATE KEY UPDATE WEATHER_STATION_KEY=WEATHER_STATION_KEY\")\n",
    "\n",
    "sqlFactWeather = (\"INSERT INTO CCC_FACT_WEATHER(DATE_KEY, WEATHER_STATION_KEY, MAX_DAILY_TEMP, MIN_DAILY_TEMP, DAILY_PRECIPITATION) \"\n",
    "                  \"(SELECT dd.DATE_KEY, wd.WEATHER_STATION_KEY, wsd.MAX_DAILY_TEMP, wsd.MIN_DAILY_TEMP, wsd.DAILY_PRECIPITATION \"\n",
    "                  \"FROM CCC_STG_WEATHER_STATION_DATA wsd \"\n",
    "                  \"JOIN CCC_DIM_DATE dd ON dd.THE_DATE = STR_TO_DATE(DATE_OF_WEATHER, '%Y%m%d') \"\n",
    "                  \"JOIN CCC_DIM_WEATHER_STATION wd ON wd.WEATHER_STATION_CODE = SUBSTR(FILE_NAME, 1,  LOCATE('.txt', FILE_NAME,1)-1)) \"\n",
    "                  \"ON DUPLICATE KEY UPDATE MAX_DAILY_TEMP = wsd.MAX_DAILY_TEMP, MIN_DAILY_TEMP = wsd.MIN_DAILY_TEMP, \"\n",
    "                  \"DAILY_PRECIPITATION = wsd.DAILY_PRECIPITATION\")\n",
    "\n",
    "sqlStgWeatherDel = (\"DELETE FROM CCC_STG_WEATHER_STATION_DATA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "This sections contains the functions used to load data into the database tables.  They are called by the next section of code in the appropriate order.  THe functions are setup to return details about the records acted on during the calls to be used for data evaluation.  The log file is also feed this information so time stamps are captured to indicate the run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadYieldFileToDF(yd, yf):\n",
    "    logging.debug('##  loadYieldFileToDF start')\n",
    "    yf_df = pd.read_csv(yd+yf,sep='\\t', names=colName1, dtype=dataType1)\n",
    "    logging.debug('##  loadYieldFileToDF end')\n",
    "    return yf_df\n",
    "\n",
    "def loadYieldFileToStage(yd, yf):\n",
    "    cnt = 0\n",
    "    logging.debug('# loadYieldFileToStage start')\n",
    "    yld_df = loadYieldFileToDF(yd, yf)\n",
    "    for index, row in yld_df.iterrows():\n",
    "        #print(row['amount'], row['year'])\n",
    "        val1 = (yf\n",
    "           , row['year']\n",
    "           , row['amount'])\n",
    "        mycursor.execute(sql1, val1)\n",
    "        cnt = cnt + mycursor.rowcount\n",
    "        mydb.commit()\n",
    "    logging.debug('# loadYieldFileToStage data frame size:'+ str(yld_df.shape))\n",
    "    logging.debug('# loadYieldFileToStage rows inserted to table:'+ str(cnt))\n",
    "    logging.debug('# loadYieldFileToStage end')\n",
    "\n",
    "    return yld_df.shape[0], cnt\n",
    "\n",
    "def loadDimYearFromYield():\n",
    "    logging.debug('# loadDimYearFromYield start')\n",
    "    mycursor.execute(sqlDimYear)\n",
    "    logging.debug('# loadDimYearFromYield rows: '+ str(mycursor.rowcount))\n",
    "    mydb.commit()\n",
    "    logging.debug('# loadDimYearFromYield start')\n",
    "    return mycursor.rowcount\n",
    "\n",
    "def loadFactYield():\n",
    "    logging.debug('# loadFactYield start')\n",
    "    mycursor.execute(sqlFactYld)\n",
    "    mydb.commit()\n",
    "    logging.debug('# loadFactYield rows: '+ str(mycursor.rowcount))\n",
    "    logging.debug('# loadFactYield end')\n",
    "    \n",
    "    return mycursor.rowcount  \n",
    "\n",
    "def loadWeatherFileToDF(wd, wf):\n",
    "    logging.debug('##  loadWeatherFileToDF start')\n",
    "    wx_df = pd.read_csv(wd+wf,sep='\\t', names=colName2, dtype=dataType2)\n",
    "    logging.debug('##  loadWeatherFileToDF end')\n",
    "    return wx_df\n",
    "\n",
    "def loadWeatherFileToStage(wd, wf):\n",
    "    cnt = 0\n",
    "    logging.debug('# loadWeatherFileToStage start')\n",
    "    wx_df = loadWeatherFileToDF(wd, wf)\n",
    "    for index, row in wx_df.iterrows():\n",
    "        val2 = (wf\n",
    "            , row['date']\n",
    "            , row['max_temp']\n",
    "            , row['min_temp']\n",
    "            , row['precip'])\n",
    "    \n",
    "        mycursor.execute(sql2, val2)\n",
    "        cnt = cnt + mycursor.rowcount\n",
    "        mydb.commit()\n",
    "    logging.debug('# loadWeatherFileToStage data frame size:'+ str(wx_df.shape))\n",
    "    logging.debug('# loadWeatherFileToStage rows: '+ str(cnt))\n",
    "    logging.debug('# loadWeatherFileToStage end')\n",
    "    return wx_df.shape[0], cnt\n",
    "\n",
    "def loadDimDateFromWeather():\n",
    "    logging.debug('# loadDimDateFromWeather start')\n",
    "    mycursor.execute(sqlDimDate)\n",
    "    logging.debug('# loadDimDateFromWeather rows:' + str(mycursor.rowcount))\n",
    "    mydb.commit()\n",
    "    logging.debug('# loadDimDateFromWeather end')\n",
    "    return mycursor.rowcount\n",
    "\n",
    "def loadDimWeatherStation():\n",
    "    logging.debug('# loadDimWeatherStation start')\n",
    "    mycursor.execute(sqlDimWeatherStation)\n",
    "    logging.debug('# loadDimWeatherStation rows: '+ str(mycursor.rowcount))\n",
    "    mydb.commit()\n",
    "    logging.debug('# loadDimWeatherStation end')\n",
    "    return mycursor.rowcount\n",
    "\n",
    "def loadFactWeather():\n",
    "    logging.debug('# loadFactWeather start')\n",
    "    mycursor.execute(sqlFactWeather)\n",
    "    logging.debug('# loadFactWeather rows: '+ str(mycursor.rowcount))\n",
    "    mydb.commit()\n",
    "    logging.debug('# loadFactWeather end')\n",
    "    \n",
    "    return mycursor.rowcount\n",
    "\n",
    "def CleanupStage():\n",
    "    logging.debug('# CleanupStage start')\n",
    "    \n",
    "    mycursor.execute(sqlStgYldDel)\n",
    "    mydb.commit()\n",
    "    logging.debug('# CleanupStage rows: '+ str(mycursor.rowcount))\n",
    "    \n",
    "    mycursor.execute(sqlStgWeatherDel)\n",
    "    mydb.commit()\n",
    "    \n",
    "    logging.debug('# CleanupStage rows: '+ str(mycursor.rowcount))\n",
    "    logging.debug('# CleanupStage end')\n",
    "    \n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Section\n",
    "In this section the file directories, data frame to capture load data, and variables used to capture load stats are established"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories for inbound file location See config file for exact settings\n",
    "yield_directory = config['DEFAULT']['yield_directory'].strip(\"'\")\n",
    "archive_directory = config['DEFAULT']['archive_directory'].strip(\"'\")\n",
    "\n",
    "ylddir = os.fsencode(yield_directory)\n",
    "\n",
    "# Create a data frame to store the results into\n",
    "result_df = pd.DataFrame(index=['1'], columns=['Run Date',\n",
    "            'Yield Files Processed','Yield Records Read','Yield Records Written to Stage',\n",
    "            'Yield Facts Written','Weather Files Processed','Weather Records Read',\n",
    "            'Weather Records Written to Stage','Weather Facts Written','Dim Year Written',\n",
    "            'Dim Dates Written','Dim Weather Stations Written'])\n",
    "\n",
    "result_df.at['1', 'Run Date'] = date.today()\n",
    "\n",
    "\n",
    "yld_records_loaded = 0\n",
    "yld_records_read = 0\n",
    "yld_files_processed = 0\n",
    "yld_year_cnt = 0\n",
    "yld_fact_cnt = 0\n",
    "\n",
    "wtr_records_loaded = 0\n",
    "wtr_records_read = 0\n",
    "wtr_files_processed = 0\n",
    "wtr_date_cnt = 0\n",
    "wtr_station_cnt = 0\n",
    "wtr_fact_cnt = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main section\n",
    "This is the main section of the code.  Here the folders where the files are input to (as defined in the configuration file) are looped through looking for appropriate named files.  In code is setup to look in specific foilders for a specific type of file.  File naming could also have been used, but for this challenge the folder was sufficient.\n",
    "\n",
    "### The flow is as follows:\n",
    "    1) load yield data file(s) into the stage table\n",
    "    2) Validate the the year dimension table contains all the years in the stage table.  Add any that are missing.\n",
    "    3) Load the yield fact table.  Utilize the logic and constriants to handle duplicates.  Update the detail data if the values change.\n",
    "\n",
    "    4) Load the weather station data into the stage table\n",
    "    5) Validate the date dimension table contains all the dates in the stage table.  Add any that are missing.\n",
    "    6) Validate the weather station dimension table contains all the weather stations based on the file names provided.  Add any that are missing.\n",
    "    7) Load the weather fact table.  Utilize the logic and constriants to handle duplicates.  Update the detail data if the values change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One - Yield File Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.debug('-------------------Data Ingestion Main start--------------------')\n",
    "\n",
    "for file in os.listdir(ylddir):\n",
    "     filename = os.fsdecode(file)\n",
    "     if filename.endswith(\".txt\"): #or filename.endswith(\".csv\"): \n",
    "         #Load file to yield tables\n",
    "         a, b = loadYieldFileToStage(yield_directory, filename)\n",
    "         yld_records_read += a\n",
    "         yld_records_loaded += b\n",
    "         os.rename(yield_directory+filename, archive_directory+filename)\n",
    "         yld_files_processed += 1\n",
    "         continue\n",
    "     else:\n",
    "         #Add comment to log file\n",
    "         continue\n",
    "# Update the result data frame with yield Load counts\n",
    "result_df.at['1', 'Yield Files Processed'] = yld_files_processed\n",
    "result_df.at['1', 'Yield Records Read'] = yld_records_read\n",
    "result_df.at['1', 'Yield Records Written to Stage'] = yld_records_loaded\n",
    "\n",
    "# Call functions to load / update the Year Dimension and Yield Fact tables\n",
    "yld_year_cnt = loadDimYearFromYield()\n",
    "yld_fact_cnt = loadFactYield()\n",
    "\n",
    "#Update the result data frame\n",
    "result_df.at['1', 'Yield Facts Written'] =  yld_fact_cnt\n",
    "result_df.at['1', 'Dim Year Written'] = yld_year_cnt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two - Weather Data File Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_directory = config['DEFAULT']['weather_directory'].strip(\"'\")\n",
    "wtrdir = os.fsencode(weather_directory)\n",
    "    \n",
    "for file in os.listdir(wtrdir):\n",
    "     filename = os.fsdecode(file)\n",
    "     if filename.endswith(\".txt\") or filename.endswith(\".csv\"): \n",
    "         #Load file to weather tables\n",
    "         a,b = loadWeatherFileToStage(weather_directory, filename)\n",
    "         wtr_records_read += a\n",
    "         wtr_records_loaded += b\n",
    "         os.rename(weather_directory+filename, archive_directory+filename)\n",
    "         wtr_files_processed += 1\n",
    "         continue\n",
    "     else:\n",
    "         #Add comment to log file\n",
    "         continue\n",
    "        \n",
    "# Update the result data frame with yield Load counts\n",
    "result_df.at['1', 'Weather Files Processed'] =wtr_files_processed\n",
    "result_df.at['1', 'Weather Records Read'] = wtr_records_read\n",
    "result_df.at['1', 'Weather Records Written to Stage'] = wtr_records_loaded\n",
    "\n",
    "\n",
    "wtr_date_cnt = loadDimDateFromWeather()\n",
    "wtr_station_cnt = loadDimWeatherStation()\n",
    "wtr_fact_cnt = loadFactWeather()\n",
    "\n",
    "#Update the result data frame\n",
    "result_df.at['1', 'Weather Facts Written'] =wtr_fact_cnt\n",
    "result_df.at['1', 'Dim Dates Written'] = wtr_date_cnt\n",
    "result_df.at['1', 'Dim Weather Stations Written'] = wtr_station_cnt\n",
    "\n",
    "logging.debug('--------------------Data Ingestion Main end------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Date</th>\n",
       "      <th>Yield Files Processed</th>\n",
       "      <th>Yield Records Read</th>\n",
       "      <th>Yield Records Written to Stage</th>\n",
       "      <th>Yield Facts Written</th>\n",
       "      <th>Weather Files Processed</th>\n",
       "      <th>Weather Records Read</th>\n",
       "      <th>Weather Records Written to Stage</th>\n",
       "      <th>Weather Facts Written</th>\n",
       "      <th>Dim Year Written</th>\n",
       "      <th>Dim Dates Written</th>\n",
       "      <th>Dim Weather Stations Written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-02</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>167</td>\n",
       "      <td>1729957</td>\n",
       "      <td>1729957</td>\n",
       "      <td>1729957</td>\n",
       "      <td>30</td>\n",
       "      <td>10957</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Run Date Yield Files Processed Yield Records Read  \\\n",
       "1  2023-04-02                     1                 30   \n",
       "\n",
       "  Yield Records Written to Stage Yield Facts Written Weather Files Processed  \\\n",
       "1                             30                  30                     167   \n",
       "\n",
       "  Weather Records Read Weather Records Written to Stage Weather Facts Written  \\\n",
       "1              1729957                          1729957               1729957   \n",
       "\n",
       "  Dim Year Written Dim Dates Written Dim Weather Stations Written  \n",
       "1               30             10957                          167  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The scope of these changes made to\n",
    "# pandas settings are local to with statement.\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup the Stage Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup the stage data\n",
    "CleanupStage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Data Load\n",
    "This section has been added to execute the call for the weather data summary by year and station for the stats API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is added to execute the code for the data analysis section\n",
    "# to load the rollup\n",
    "rollup_sql = (\"INSERT INTO CCC_FACT_WEATHER_ROLLUP(YEAR_KEY, WEATHER_STATION_KEY, AVG_MAX_TEMP, AVG_MIN_TEMP, TOT_PRECIPITATION) \"\n",
    "            \" WITH cte AS \"\n",
    "            \" (SELECT year_key,weather_station_key, \"\n",
    "            \" AVG(IF(max_daily_temp = -9999, null, max_daily_temp))/10 as avg_max, \"\n",
    "            \" AVG(IF(min_daily_temp = -9999, null, min_daily_temp))/10 as avg_min, \"\n",
    "            \" SUM(IF(daily_precipitation = -9999, null, daily_precipitation))/10/10 as tot_prec \"\n",
    "            \" FROM CCC_FACT_WEATHER fw \"\n",
    "            \" JOIN CCC_DIM_DATE dd ON fw.date_key = dd.date_key \"\n",
    "            \" JOIN CCC_DIM_YEAR dy ON dd.the_year = dy.the_year \"\n",
    "            \" GROUP BY year_key, weather_station_key) \"\n",
    "            \" (SELECT year_key,weather_station_key, avg_max, avg_min, tot_prec \"\n",
    "            \" FROM cte) \"\n",
    "            \" ON DUPLICATE KEY UPDATE AVG_MAX_TEMP=cte.avg_max, \"\n",
    "            \" AVG_MIN_TEMP=cte.avg_min, \"\n",
    "            \" TOT_PRECIPITATION = cte.tot_prec\")\n",
    "mycursor.execute(rollup_sql)\n",
    "mydb.commit()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close\n",
    "Close out the cursor and db connection before we leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close cursor and db connection\n",
    "mycursor.close()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
